{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    lrate = 0.001\n",
    "    if epoch > 75:\n",
    "        lrate = 0.0005\n",
    "    if epoch > 100:\n",
    "        lrate = 0.0003\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 784s 5us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    " \n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    " \n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    )\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 1.9697 - acc: 0.4149 - val_loss: 1.4696 - val_acc: 0.5469\n",
      "Epoch 2/125\n",
      "781/781 [==============================] - 23s 30ms/step - loss: 1.4285 - acc: 0.5716 - val_loss: 1.4606 - val_acc: 0.6278\n",
      "Epoch 3/125\n",
      "781/781 [==============================] - 23s 30ms/step - loss: 1.3590 - acc: 0.6161 - val_loss: 1.2616 - val_acc: 0.6592\n",
      "Epoch 4/125\n",
      "781/781 [==============================] - 23s 30ms/step - loss: 1.2393 - acc: 0.6504 - val_loss: 1.0395 - val_acc: 0.7013\n",
      "Epoch 5/125\n",
      "781/781 [==============================] - 23s 30ms/step - loss: 1.2141 - acc: 0.6704 - val_loss: 0.9787 - val_acc: 0.7123\n",
      "Epoch 6/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 1.1661 - acc: 0.6875 - val_loss: 0.9976 - val_acc: 0.7244\n",
      "Epoch 7/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 1.0612 - acc: 0.7146 - val_loss: 0.9631 - val_acc: 0.7402\n",
      "Epoch 8/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 1.0406 - acc: 0.7252 - val_loss: 0.9312 - val_acc: 0.7465\n",
      "Epoch 9/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 1.0635 - acc: 0.7297 - val_loss: 1.0480 - val_acc: 0.7610\n",
      "Epoch 10/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.9993 - acc: 0.7373 - val_loss: 0.9153 - val_acc: 0.7633\n",
      "Epoch 11/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.9768 - acc: 0.7451 - val_loss: 0.7854 - val_acc: 0.7805\n",
      "Epoch 12/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.9809 - acc: 0.7513 - val_loss: 0.8135 - val_acc: 0.7915\n",
      "Epoch 13/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.9458 - acc: 0.7607 - val_loss: 0.8127 - val_acc: 0.7836\n",
      "Epoch 14/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.9328 - acc: 0.7626 - val_loss: 0.8556 - val_acc: 0.7792\n",
      "Epoch 15/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.9208 - acc: 0.7665 - val_loss: 0.8075 - val_acc: 0.7913\n",
      "Epoch 16/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.9084 - acc: 0.7735 - val_loss: 0.7793 - val_acc: 0.8015\n",
      "Epoch 17/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.9170 - acc: 0.7757 - val_loss: 0.7353 - val_acc: 0.8059\n",
      "Epoch 18/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.8781 - acc: 0.7793 - val_loss: 0.8289 - val_acc: 0.7970\n",
      "Epoch 19/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.8770 - acc: 0.7827 - val_loss: 0.7599 - val_acc: 0.8020\n",
      "Epoch 20/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.8674 - acc: 0.7848 - val_loss: 0.7693 - val_acc: 0.8038\n",
      "Epoch 21/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.8690 - acc: 0.7824 - val_loss: 0.8056 - val_acc: 0.7947\n",
      "Epoch 22/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.8514 - acc: 0.7881 - val_loss: 0.7202 - val_acc: 0.8209\n",
      "Epoch 23/125\n",
      "781/781 [==============================] - 24s 30ms/step - loss: 0.8479 - acc: 0.7872 - val_loss: 0.8151 - val_acc: 0.7927\n",
      "Epoch 24/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.8456 - acc: 0.7903 - val_loss: 0.6973 - val_acc: 0.8272\n",
      "Epoch 25/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.8161 - acc: 0.7943 - val_loss: 0.7972 - val_acc: 0.7975\n",
      "Epoch 26/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.7968 - acc: 0.8002 - val_loss: 0.6919 - val_acc: 0.8305\n",
      "Epoch 27/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.7988 - acc: 0.7988 - val_loss: 0.7888 - val_acc: 0.7956\n",
      "Epoch 28/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.7829 - acc: 0.8036 - val_loss: 0.7350 - val_acc: 0.8095\n",
      "Epoch 29/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.7887 - acc: 0.8026 - val_loss: 0.8060 - val_acc: 0.7986\n",
      "Epoch 30/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.7597 - acc: 0.8056 - val_loss: 0.7156 - val_acc: 0.8284\n",
      "Epoch 31/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.7475 - acc: 0.8082 - val_loss: 0.7832 - val_acc: 0.8046\n",
      "Epoch 32/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.7407 - acc: 0.8090 - val_loss: 0.7064 - val_acc: 0.8315\n",
      "Epoch 33/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.7307 - acc: 0.8091 - val_loss: 0.6544 - val_acc: 0.8412\n",
      "Epoch 34/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.7215 - acc: 0.8120 - val_loss: 0.6355 - val_acc: 0.8449\n",
      "Epoch 35/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.7019 - acc: 0.8161 - val_loss: 0.7489 - val_acc: 0.8094\n",
      "Epoch 36/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6980 - acc: 0.8165 - val_loss: 0.7312 - val_acc: 0.8198\n",
      "Epoch 37/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.6941 - acc: 0.8188 - val_loss: 0.6953 - val_acc: 0.8223\n",
      "Epoch 38/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6857 - acc: 0.8194 - val_loss: 0.7322 - val_acc: 0.8174\n",
      "Epoch 39/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.6766 - acc: 0.8223 - val_loss: 0.7579 - val_acc: 0.8096\n",
      "Epoch 40/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.6695 - acc: 0.8244 - val_loss: 0.6720 - val_acc: 0.8290\n",
      "Epoch 41/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.6684 - acc: 0.8238 - val_loss: 0.7009 - val_acc: 0.8238\n",
      "Epoch 42/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6611 - acc: 0.8273 - val_loss: 0.6812 - val_acc: 0.8248\n",
      "Epoch 43/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6628 - acc: 0.8252 - val_loss: 0.6533 - val_acc: 0.8371\n",
      "Epoch 44/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.6549 - acc: 0.8263 - val_loss: 0.6389 - val_acc: 0.8393\n",
      "Epoch 45/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6489 - acc: 0.8297 - val_loss: 0.6851 - val_acc: 0.8282\n",
      "Epoch 46/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.6413 - acc: 0.8314 - val_loss: 0.6453 - val_acc: 0.8415\n",
      "Epoch 47/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6415 - acc: 0.8307 - val_loss: 0.6196 - val_acc: 0.8473\n",
      "Epoch 48/125\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 0.6414 - acc: 0.8314 - val_loss: 0.6783 - val_acc: 0.8306\n",
      "Epoch 49/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.6402 - acc: 0.8318 - val_loss: 0.6116 - val_acc: 0.8525\n",
      "Epoch 50/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.6319 - acc: 0.8355 - val_loss: 0.6650 - val_acc: 0.8354\n",
      "Epoch 51/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.6278 - acc: 0.8359 - val_loss: 0.6613 - val_acc: 0.8360\n",
      "Epoch 52/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.6280 - acc: 0.8364 - val_loss: 0.5942 - val_acc: 0.8573\n",
      "Epoch 53/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6221 - acc: 0.8371 - val_loss: 0.6094 - val_acc: 0.8529\n",
      "Epoch 54/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6229 - acc: 0.8382 - val_loss: 0.6804 - val_acc: 0.8361\n",
      "Epoch 55/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6224 - acc: 0.8383 - val_loss: 0.6302 - val_acc: 0.8524\n",
      "Epoch 56/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6176 - acc: 0.8402 - val_loss: 0.6879 - val_acc: 0.8297\n",
      "Epoch 57/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6171 - acc: 0.8404 - val_loss: 0.5677 - val_acc: 0.8601\n",
      "Epoch 58/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6146 - acc: 0.8387 - val_loss: 0.6024 - val_acc: 0.8486\n",
      "Epoch 59/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6118 - acc: 0.8411 - val_loss: 0.6338 - val_acc: 0.8462\n",
      "Epoch 60/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6144 - acc: 0.8405 - val_loss: 0.6079 - val_acc: 0.8552\n",
      "Epoch 61/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6115 - acc: 0.8415 - val_loss: 0.7086 - val_acc: 0.8218\n",
      "Epoch 62/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6088 - acc: 0.8433 - val_loss: 0.6335 - val_acc: 0.8441\n",
      "Epoch 63/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.6110 - acc: 0.8417 - val_loss: 0.6492 - val_acc: 0.8386\n",
      "Epoch 64/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6045 - acc: 0.8430 - val_loss: 0.6133 - val_acc: 0.8419\n",
      "Epoch 65/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6039 - acc: 0.8447 - val_loss: 0.5886 - val_acc: 0.8549\n",
      "Epoch 66/125\n",
      "781/781 [==============================] - 24s 30ms/step - loss: 0.6036 - acc: 0.8442 - val_loss: 0.5943 - val_acc: 0.8556\n",
      "Epoch 67/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.5999 - acc: 0.8442 - val_loss: 0.5834 - val_acc: 0.8614\n",
      "Epoch 68/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6044 - acc: 0.8435 - val_loss: 0.6455 - val_acc: 0.8375\n",
      "Epoch 69/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6013 - acc: 0.8466 - val_loss: 0.5955 - val_acc: 0.8551\n",
      "Epoch 70/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6027 - acc: 0.8446 - val_loss: 0.6537 - val_acc: 0.8412\n",
      "Epoch 71/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.5938 - acc: 0.8476 - val_loss: 0.6135 - val_acc: 0.8487\n",
      "Epoch 72/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.6012 - acc: 0.8446 - val_loss: 0.5803 - val_acc: 0.8593\n",
      "Epoch 73/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.5942 - acc: 0.8486 - val_loss: 0.5966 - val_acc: 0.8552\n",
      "Epoch 74/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.5985 - acc: 0.8474 - val_loss: 0.5686 - val_acc: 0.8651\n",
      "Epoch 75/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.5964 - acc: 0.8482 - val_loss: 0.6602 - val_acc: 0.8404\n",
      "Epoch 76/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.5957 - acc: 0.8477 - val_loss: 0.6593 - val_acc: 0.8334\n",
      "Epoch 77/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.5497 - acc: 0.8621 - val_loss: 0.5730 - val_acc: 0.8653\n",
      "Epoch 78/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.5345 - acc: 0.8675 - val_loss: 0.5380 - val_acc: 0.8722\n",
      "Epoch 79/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.5226 - acc: 0.8704 - val_loss: 0.5316 - val_acc: 0.8760\n",
      "Epoch 80/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.5196 - acc: 0.8699 - val_loss: 0.5251 - val_acc: 0.8734\n",
      "Epoch 81/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.5110 - acc: 0.8715 - val_loss: 0.6050 - val_acc: 0.8531\n",
      "Epoch 82/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.5077 - acc: 0.8728 - val_loss: 0.5259 - val_acc: 0.8707\n",
      "Epoch 83/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.5031 - acc: 0.8735 - val_loss: 0.5219 - val_acc: 0.8758\n",
      "Epoch 84/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.5024 - acc: 0.8723 - val_loss: 0.5535 - val_acc: 0.8660\n",
      "Epoch 85/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.5004 - acc: 0.8725 - val_loss: 0.5406 - val_acc: 0.8669\n",
      "Epoch 86/125\n",
      "781/781 [==============================] - 24s 30ms/step - loss: 0.4931 - acc: 0.8742 - val_loss: 0.5392 - val_acc: 0.8649\n",
      "Epoch 87/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4884 - acc: 0.8748 - val_loss: 0.5199 - val_acc: 0.8727\n",
      "Epoch 88/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4887 - acc: 0.8753 - val_loss: 0.5107 - val_acc: 0.8774\n",
      "Epoch 89/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4873 - acc: 0.8757 - val_loss: 0.5140 - val_acc: 0.8741\n",
      "Epoch 90/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4823 - acc: 0.8764 - val_loss: 0.5319 - val_acc: 0.8671\n",
      "Epoch 91/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4846 - acc: 0.8759 - val_loss: 0.5100 - val_acc: 0.8763\n",
      "Epoch 92/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4811 - acc: 0.8767 - val_loss: 0.5401 - val_acc: 0.8675\n",
      "Epoch 93/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4818 - acc: 0.8754 - val_loss: 0.5006 - val_acc: 0.8740\n",
      "Epoch 94/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4816 - acc: 0.8753 - val_loss: 0.5185 - val_acc: 0.8711\n",
      "Epoch 95/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4777 - acc: 0.8761 - val_loss: 0.4981 - val_acc: 0.8791\n",
      "Epoch 96/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4788 - acc: 0.8761 - val_loss: 0.4935 - val_acc: 0.8762\n",
      "Epoch 97/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4739 - acc: 0.8774 - val_loss: 0.4995 - val_acc: 0.8769\n",
      "Epoch 98/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4755 - acc: 0.8754 - val_loss: 0.4830 - val_acc: 0.8806\n",
      "Epoch 99/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4751 - acc: 0.8773 - val_loss: 0.5001 - val_acc: 0.8785\n",
      "Epoch 100/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4714 - acc: 0.8763 - val_loss: 0.4672 - val_acc: 0.8846\n",
      "Epoch 101/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4720 - acc: 0.8773 - val_loss: 0.4919 - val_acc: 0.8782\n",
      "Epoch 102/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4518 - acc: 0.8841 - val_loss: 0.4691 - val_acc: 0.8843\n",
      "Epoch 103/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4491 - acc: 0.8847 - val_loss: 0.4989 - val_acc: 0.8755\n",
      "Epoch 104/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4381 - acc: 0.8883 - val_loss: 0.5174 - val_acc: 0.8749\n",
      "Epoch 105/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4315 - acc: 0.8908 - val_loss: 0.4803 - val_acc: 0.8821\n",
      "Epoch 106/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4338 - acc: 0.8887 - val_loss: 0.4974 - val_acc: 0.8771\n",
      "Epoch 107/125\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 0.4272 - acc: 0.8905 - val_loss: 0.4730 - val_acc: 0.8842\n",
      "Epoch 108/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.4255 - acc: 0.8908 - val_loss: 0.4831 - val_acc: 0.8801\n",
      "Epoch 109/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.4292 - acc: 0.8877 - val_loss: 0.4856 - val_acc: 0.8783\n",
      "Epoch 110/125\n",
      "781/781 [==============================] - 21s 28ms/step - loss: 0.4286 - acc: 0.8891 - val_loss: 0.5082 - val_acc: 0.8789\n",
      "Epoch 111/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.4239 - acc: 0.8897 - val_loss: 0.4621 - val_acc: 0.8877\n",
      "Epoch 112/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.4240 - acc: 0.8887 - val_loss: 0.4771 - val_acc: 0.8824\n",
      "Epoch 113/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.4197 - acc: 0.8916 - val_loss: 0.4778 - val_acc: 0.8821\n",
      "Epoch 114/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.4181 - acc: 0.8924 - val_loss: 0.4895 - val_acc: 0.8771\n",
      "Epoch 115/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4153 - acc: 0.8927 - val_loss: 0.4927 - val_acc: 0.8790\n",
      "Epoch 116/125\n",
      "781/781 [==============================] - 21s 28ms/step - loss: 0.4171 - acc: 0.8920 - val_loss: 0.4582 - val_acc: 0.8868\n",
      "Epoch 117/125\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.4135 - acc: 0.8924 - val_loss: 0.4511 - val_acc: 0.8887\n",
      "Epoch 118/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.4154 - acc: 0.8917 - val_loss: 0.4667 - val_acc: 0.8847\n",
      "Epoch 119/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.4088 - acc: 0.8936 - val_loss: 0.4732 - val_acc: 0.8800\n",
      "Epoch 120/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.4153 - acc: 0.8919 - val_loss: 0.4523 - val_acc: 0.8876\n",
      "Epoch 121/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 21s 27ms/step - loss: 0.4138 - acc: 0.8937 - val_loss: 0.4876 - val_acc: 0.8783\n",
      "Epoch 122/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.4057 - acc: 0.8933 - val_loss: 0.4747 - val_acc: 0.8805\n",
      "Epoch 123/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.4083 - acc: 0.8929 - val_loss: 0.4567 - val_acc: 0.8858\n",
      "Epoch 124/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.4098 - acc: 0.8925 - val_loss: 0.4637 - val_acc: 0.8839\n",
      "Epoch 125/125\n",
      "781/781 [==============================] - 21s 27ms/step - loss: 0.4101 - acc: 0.8926 - val_loss: 0.4861 - val_acc: 0.8760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b61d685e48>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\\\n",
    "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 103us/step\n",
      "\n",
      "Test result: 87.600 loss: 0.486\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
